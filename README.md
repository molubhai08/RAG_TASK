# Policy Document RAG System ðŸ“„ðŸ¤–

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline for answering questions strictly based on **company policy PDFs**. It is designed to minimize hallucinations and provide **fact-based, document-grounded answers** using modern NLP tooling.

---

## âœ¨ Features

* ðŸ“‚ Loads and processes multi-page PDF policy documents
* ðŸ§© Automatically detects and chunks **policy sections**
* ðŸ“ Dynamically selects optimal chunk size using token statistics
* ðŸ” Semantic search with **FAISS + HuggingFace embeddings**
* ðŸ§  Uses **Groq LLaMA 3.3 (70B)** for accurate, low-temperature responses
* ðŸš« Built-in refusal mechanism for unanswerable questions
* ðŸŽ¯ Strict prompt rules to prevent hallucinations

---

## ðŸ—ï¸ Architecture Overview

```
PDF â†’ Section Split â†’ Token Analysis â†’ Smart Chunking
    â†’ Embeddings â†’ FAISS Vector Store
    â†’ Gated Retrieval â†’ LLM (Groq) â†’ Answer
```

---

## ðŸ› ï¸ Tech Stack

* **Python**
* **LangChain**
* **FAISS** (Vector Database)
* **HuggingFace Sentence Transformers**
* **Groq API (LLaMAâ€‘3.3â€‘70B)**
* **tiktoken** (Token analysis)
* **dotenv** (Environment variables)

---

## ðŸ“ Project Structure

```
app.py                # Main RAG pipeline
Company Policies.pdf  # Input policy document
.env                  # Environment variables (GROQ_API_KEY)
README.md             # Project documentation
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install Dependencies

```bash
pip install langchain langchain-community langchain-groq \
            sentence-transformers faiss-cpu tiktoken python-dotenv
```

---

### 2ï¸âƒ£ Add Environment Variables

Create a `.env` file:

```env
GROQ_API_KEY=your_groq_api_key_here
```

---

### 3ï¸âƒ£ Add Policy Document

Place your PDF file in the root directory and update the filename if needed:

```python
pdf_path = "Company Policies.pdf"
```

---

### 4ï¸âƒ£ Run the Application

```bash
python app.py
```

---

## ðŸ” How It Works (Stepâ€‘byâ€‘Step)

### ðŸ“Œ 1. PDF Loading

* Uses `PyPDFLoader` to extract text from each page

---

### ðŸ“Œ 2. Sectionâ€‘Based Chunking

* Splits content using regex matching:

  ```
  1. XYZ Policy
  ```
* Filters out very small or noisy sections

---

### ðŸ“Œ 3. Token Statistics & Dynamic Chunking

* Calculates:

  * Min / Avg / Median / Max tokens
  * 75th percentile
* Automatically sets:

  * `CHUNK_SIZE â‰ˆ min(75th percentile, 600)`
  * `CHUNK_OVERLAP = 20%`

---

### ðŸ“Œ 4. Embedding & Vector Storage

* Model: `all-MiniLM-L6-v2`
* Normalized embeddings for cosine similarity
* Stored using FAISS

---

### ðŸ“Œ 5. Gated Retrieval Logic

Only returns chunks when:

* Best similarity score â‰¥ `0.2`
* Other chunks â‰¥ `60%` of best score

This avoids irrelevant context injection.

---

### ðŸ“Œ 6. Strict RAG Prompting

The LLM **must**:

* Answer only from retrieved context
* Clearly state missing information
* Refuse when context is insufficient

Refusal message:

```
I cannot answer this question based on the provided documents.
```

---

## ðŸ§ª Example Queries

âœ”ï¸ Answerable:

* "What are the delivery timelines for express shipping?"

âš ï¸ Partially Answerable:

* "What factors can delay international shipping?"

âŒ Unanswerable:

* "Which cities are part of the sameâ€‘day delivery pilot?"

---

## ðŸ” Hallucination Control Measures

* Temperature set to `0.0`
* Retrieval score gating
* Contextâ€‘only prompt rules
* Explicit refusal handling

---

## ðŸš€ Use Cases

* Internal policy assistants
* Compliance Q&A bots
* Employee selfâ€‘service tools
* Enterprise RAG prototypes

---

## ðŸ“Œ Notes

* Designed for **accuracy > creativity**
* Works best with structured policy documents
* Easily extensible to other PDFs or domains

---

